[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "watsy0007",
    "section": "",
    "text": "Analyzing Blockchain Data with DuckDB: Data Preparation\n\n\n7 min\n\n\n\n\n\n\nAug 10, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfind missing dates with DuckDB\n\n\n5 min\n\n\n\n\n\n\nJul 26, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDuckDB Example\n\n\n1 min\n\n\n\n\n\n\nFeb 17, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/find_missing_dates_with_duckdb/index.html",
    "href": "blog/find_missing_dates_with_duckdb/index.html",
    "title": "find missing dates with DuckDB",
    "section": "",
    "text": "Recently, the business feedback that there is data missing, and it is necessary to locate the missing dates in order to supplement historical data.\nAfter research, it was decided to use the Gantt chart to display the daily execution of all tasks."
  },
  {
    "objectID": "blog/find_missing_dates_with_duckdb/index.html#get-data",
    "href": "blog/find_missing_dates_with_duckdb/index.html#get-data",
    "title": "find missing dates with DuckDB",
    "section": "1. Get data",
    "text": "1. Get data\nHere use mock data\n\n\nCode\nimport duckdb\n\nconn = duckdb.connect()\n\nmock_data_sql = \"\"\"\nSELECT * FROM (\n    VALUES \n      (1, DATE '2024-07-01'),\n      (1, DATE '2024-07-02'),\n      (1, DATE '2024-07-03'),\n      (1, DATE '2024-07-05'),\n      (1, DATE '2024-07-06'),\n      (2, DATE '2024-07-01'),\n      (2, DATE '2024-07-02'),\n      (2, DATE '2024-07-03'),\n      (3, DATE '2024-07-01'),\n      (3, DATE '2024-07-02'),\n      (3, DATE '2024-07-03'),\n      (3, DATE '2024-07-04'),\n      (3, DATE '2024-07-05'),\n      (3, DATE '2024-07-06'),\n      (3, DATE '2024-07-07'),\n      (3, DATE '2024-07-08'),\n      (3, DATE '2024-07-09'),\n      (4, DATE '2024-07-05'),\n      (4, DATE '2024-07-06'),\n      (4, DATE '2024-07-07'),\n      (4, DATE '2024-07-08'),\n      (4, DATE '2024-07-09'),\n  ) AS t(source_id, end_date)\n\"\"\"\ndf = conn.execute(mock_data_sql).df()\ndf\n\n\n\n\n\n\n\n\n\nsource_id\nend_date\n\n\n\n\n0\n1\n2024-07-01\n\n\n1\n1\n2024-07-02\n\n\n2\n1\n2024-07-03\n\n\n3\n1\n2024-07-05\n\n\n4\n1\n2024-07-06\n\n\n5\n2\n2024-07-01\n\n\n6\n2\n2024-07-02\n\n\n7\n2\n2024-07-03\n\n\n8\n3\n2024-07-01\n\n\n9\n3\n2024-07-02\n\n\n10\n3\n2024-07-03\n\n\n11\n3\n2024-07-04\n\n\n12\n3\n2024-07-05\n\n\n13\n3\n2024-07-06\n\n\n14\n3\n2024-07-07\n\n\n15\n3\n2024-07-08\n\n\n16\n3\n2024-07-09\n\n\n17\n4\n2024-07-05\n\n\n18\n4\n2024-07-06\n\n\n19\n4\n2024-07-07\n\n\n20\n4\n2024-07-08\n\n\n21\n4\n2024-07-09"
  },
  {
    "objectID": "blog/find_missing_dates_with_duckdb/index.html#group-by-date",
    "href": "blog/find_missing_dates_with_duckdb/index.html#group-by-date",
    "title": "find missing dates with DuckDB",
    "section": "2. Group by date",
    "text": "2. Group by date\nThe main difficulty in finding missing dates is to group by time continuity, continuous time is placed in the same group, so if a source_id has multiple time periods, it means that there is a time gap.\nUse window functions to process the time of the current row, and get the grouping time of the current row based on the time difference. The code is as follows:\n\ngroup_date_sql = \"\"\"\n SELECT \n    source_id,\n    end_date,\n    end_date - INTERVAL (ROW_NUMBER() OVER (PARTITION BY source_id ORDER BY end_date) - 1) DAY AS group_date\n  FROM df\n  order by source_id, end_date\n\"\"\"\ngrouped_date_df = conn.execute(group_date_sql).df()\ngrouped_date_df\n\n\n\n\n\n\n\n\nsource_id\nend_date\ngroup_date\n\n\n\n\n0\n1\n2024-07-01\n2024-07-01\n\n\n1\n1\n2024-07-02\n2024-07-01\n\n\n2\n1\n2024-07-03\n2024-07-01\n\n\n3\n1\n2024-07-05\n2024-07-02\n\n\n4\n1\n2024-07-06\n2024-07-02\n\n\n5\n2\n2024-07-01\n2024-07-01\n\n\n6\n2\n2024-07-02\n2024-07-01\n\n\n7\n2\n2024-07-03\n2024-07-01\n\n\n8\n3\n2024-07-01\n2024-07-01\n\n\n9\n3\n2024-07-02\n2024-07-01\n\n\n10\n3\n2024-07-03\n2024-07-01\n\n\n11\n3\n2024-07-04\n2024-07-01\n\n\n12\n3\n2024-07-05\n2024-07-01\n\n\n13\n3\n2024-07-06\n2024-07-01\n\n\n14\n3\n2024-07-07\n2024-07-01\n\n\n15\n3\n2024-07-08\n2024-07-01\n\n\n16\n3\n2024-07-09\n2024-07-01\n\n\n17\n4\n2024-07-05\n2024-07-05\n\n\n18\n4\n2024-07-06\n2024-07-05\n\n\n19\n4\n2024-07-07\n2024-07-05\n\n\n20\n4\n2024-07-08\n2024-07-05\n\n\n21\n4\n2024-07-09\n2024-07-05"
  },
  {
    "objectID": "blog/find_missing_dates_with_duckdb/index.html#group-by-source_id-and-group-date",
    "href": "blog/find_missing_dates_with_duckdb/index.html#group-by-source_id-and-group-date",
    "title": "find missing dates with DuckDB",
    "section": "3. Group by source_id and group date",
    "text": "3. Group by source_id and group date\n\ngroup_sql = \"\"\"\nSELECT \n    source_id,\n    MIN(end_date) AS start_date,\n    MAX(end_date) AS end_date,\n  FROM grouped_date_df\n  GROUP BY source_id, group_date\n  ORDER BY source_id, group_date\n\"\"\"\ngrouped_df = conn.execute(group_sql).df()\ngrouped_df\n\n\n\n\n\n\n\n\nsource_id\nstart_date\nend_date\n\n\n\n\n0\n1\n2024-07-01\n2024-07-03\n\n\n1\n1\n2024-07-05\n2024-07-06\n\n\n2\n2\n2024-07-01\n2024-07-03\n\n\n3\n3\n2024-07-01\n2024-07-09\n\n\n4\n4\n2024-07-05\n2024-07-09"
  },
  {
    "objectID": "blog/find_missing_dates_with_duckdb/index.html#visualization",
    "href": "blog/find_missing_dates_with_duckdb/index.html#visualization",
    "title": "find missing dates with DuckDB",
    "section": "4. Visualization",
    "text": "4. Visualization\n\n\nCode\nimport plotly.express as px\nfig = px.timeline(grouped_df, x_start='start_date', x_end='end_date', y='source_id')\nfig.update_yaxes(autorange=\"reversed\")\nfig.show()"
  },
  {
    "objectID": "blog/find_missing_dates_with_duckdb/index.html#full-code",
    "href": "blog/find_missing_dates_with_duckdb/index.html#full-code",
    "title": "find missing dates with DuckDB",
    "section": "5. Full code",
    "text": "5. Full code\n\nsql = \"\"\"\nwith raw_data as (\n  SELECT * FROM (\n    VALUES \n      (1, DATE '2024-07-01'),\n      (1, DATE '2024-07-02'),\n      (1, DATE '2024-07-03'),\n      (1, DATE '2024-07-05'),\n      (1, DATE '2024-07-06'),\n      (2, DATE '2024-07-01'),\n      (2, DATE '2024-07-02'),\n      (2, DATE '2024-07-03'),\n      (3, DATE '2024-07-01'),\n      (3, DATE '2024-07-02'),\n      (3, DATE '2024-07-03'),\n      (3, DATE '2024-07-04'),\n      (3, DATE '2024-07-05'),\n      (3, DATE '2024-07-06'),\n      (3, DATE '2024-07-07'),\n      (3, DATE '2024-07-08'),\n      (3, DATE '2024-07-09'),\n      (4, DATE '2024-07-05'),\n      (4, DATE '2024-07-06'),\n      (4, DATE '2024-07-07'),\n      (4, DATE '2024-07-08'),\n      (4, DATE '2024-07-09'),\n  ) AS t(source_id, end_date)\n), group_date as (\n  SELECT \n    source_id,\n    end_date,\n    end_date - INTERVAL (ROW_NUMBER() OVER (PARTITION BY source_id ORDER BY end_date) - 1) DAY AS group_date\n  FROM raw_data\n  order by source_id, end_date\n), final as (\n  SELECT \n    source_id,\n    MIN(end_date) AS start_date,\n    MAX(end_date) AS end_date,\n  FROM grouped_date_df\n  GROUP BY source_id, group_date\n  ORDER BY source_id, group_date\n)\nfrom final\n\"\"\"\ndate_df = conn.execute(sql).df()\ngap_fig = px.timeline(date_df, x_start='start_date', x_end='end_date', y='source_id')\ngap_fig.update_yaxes(autorange=\"reversed\")\ngap_fig.show()"
  },
  {
    "objectID": "blog/analyzing_blockchain_data_with_duckdb_1/index.html",
    "href": "blog/analyzing_blockchain_data_with_duckdb_1/index.html",
    "title": "Analyzing Blockchain Data with DuckDB: Data Preparation",
    "section": "",
    "text": "I’ve been using DuckDB as a replacement for pandas and Python for data processing tasks. It’s proven to be incredibly convenient.\nA friend recently asked about my use of DuckDB in daily work, which inspired me to write a series of articles. This is the first in that series, focusing on how to use DuckDB for intial data processing."
  },
  {
    "objectID": "blog/analyzing_blockchain_data_with_duckdb_1/index.html#solution-1-explorer-implementation",
    "href": "blog/analyzing_blockchain_data_with_duckdb_1/index.html#solution-1-explorer-implementation",
    "title": "Analyzing Blockchain Data with DuckDB: Data Preparation",
    "section": "Solution 1: Explorer Implementation",
    "text": "Solution 1: Explorer Implementation\nThis approach focuses on quickly retrieving and processing transaction data. It’s ideal for rapid analysis and verification during the development stage.\nGet the transaction information of the ETH address through the blockscout API4, the code is as follows:\n\n\nCode\ndef blockscout_api(module: str, action: str, address: str, start_block: int, end_block: int, page: int, offset: int) -&gt; list[str]:\n    url_prefix = f'https://eth.blockscout.com/api?module={module}&action={action}'\n    \n    result = []\n    while True:\n        url = f'{url_prefix}&address={address}&startblock={start_block}&endblock={end_block}&page={page}&offset={offset}&sort=asc'\n        print(f'query page {page}')\n        data = requests.get(url).json()\n        if data['message'] == 'OK':\n            items = data['result']\n            result.extend(map(json.dumps,items))\n        else:\n            break\n        if len(items) &lt; offset:\n            break\n        page += 1\n    return result\n\n\nRegister the custom function of DuckDB\n\nconn = duckdb.connect()\nconn = conn.create_function('blockscout_api', blockscout_api)\n\nDefine the macro of DuckDB, here for demonstration, limit the page and offset, and adjust according to the actual situation when actually using. Note the output query page 1 and query page 2 below\n\nconn.execute(\"\"\"\nCREATE OR REPLACE MACRO blockscout_trxs(address, start_block, end_block) as table \n    select blockscout_api('account', 'txlist', address, start_block, end_block, 1, 2) as data\n\"\"\")\n\nQuery the transaction information of the ETH address\n\nconn.execute(\"\"\"\nwith raw_transactions as (\n    select unnest(data) as trx from blockscout_trxs('0x603602E9A2ac7f1E26717C2b2193Fd68f5fafFf6', 20485198, 20490674)\n), decode_transactions as (\nselect \n    trx-&gt;'$.blockHash' as block_hash,\n    (trx-&gt;'$.blockNumber')::integer as block_number,\n    (trx-&gt;'$.timeStamp')::integer as timestamp,\n    to_timestamp(timestamp) as datetime,\n    trx-&gt;'$.hash' as hash,\n    (trx-&gt;'$.transactionIndex')::integer as transaction_index,\n    trx-&gt;'$.from' as 'from',\n    trx-&gt;'$.to' as 'to',\n    trx-&gt;'$.value' as value,\n    trx-&gt;'$.contractAddress' as contract_address,\n    (trx-&gt;'$.gas')::integer as gas,\n    (trx-&gt;'$.gasPrice')::bigint as gas_price,\n    (trx-&gt;'$.gasUsed')::integer as gas_used,\n    trx-&gt;'$.isError' as is_error,\n    trx-&gt;'$.txreceipt_status' as txreceipt_status,\n    trx-&gt;'input' as 'input'\nfrom raw_transactions\n)\nselect \n  block_number,\n  datetime,\n  hash,\n  'from',\n  'to',\n  value,\nfrom decode_transactions\n\"\"\").df()\n\nquery page 1\nquery page 2\n\n\n\n\n\n\n\n\n\nblock_number\ndatetime\nhash\n'from'\n'to'\nvalue\n\n\n\n\n0\n20485198\n2024-08-09 00:55:23+08:00\n\"0x16e9d0643ce6bf9bc59d5e6c756a196af2941cefc46...\nfrom\nto\n\"500000000000000000\"\n\n\n1\n20488106\n2024-08-09 10:38:47+08:00\n\"0x3f29ab5ba5779df75aee038cb9d529ab7d7e94ff727...\nfrom\nto\n\"500000000000000000\"\n\n\n2\n20490674\n2024-08-09 19:14:23+08:00\n\"0xcba85af304112c712c978968ff19fb150cdfd18e1f4...\nfrom\nto\n\"200000000000000000\""
  },
  {
    "objectID": "blog/analyzing_blockchain_data_with_duckdb_1/index.html#solution-2-advanced-implementation-with-field-constraints",
    "href": "blog/analyzing_blockchain_data_with_duckdb_1/index.html#solution-2-advanced-implementation-with-field-constraints",
    "title": "Analyzing Blockchain Data with DuckDB: Data Preparation",
    "section": "Solution 2: Advanced Implementation with Field constraints",
    "text": "Solution 2: Advanced Implementation with Field constraints\nThis solution is more robust, suitable for production environments. It addresses potential issues like API field changes and null values in the returned data.\nDeclare the required fields and types\n\nfields = {\n    'blockHash': str,\n    'blockNumber': int,\n    'timeStamp': int,\n    'hash': str,\n    'transactionIndex': int,\n    'from': str,\n    'to': str,\n    'value': str,\n    'contractAddress': str,\n    'gas': int,\n    'gasPrice': int,\n    'gasUsed': int,\n    'isError': int,\n    'txreceipt_status': int,\n    'input': str,\n}\n\nRequest the blockscout API5 and extract valid fields\n\n\nCode\nfield_keys = fields.keys()\n\ndef blockscout_api_with_fields(module: str, action: str, address: str, start_block: int, end_block: int, page: int, offset: int):\n    url_prefix = f'https://eth.blockscout.com/api?module={module}&action={action}'\n    result = []\n    while True:\n        url = f'{url_prefix}&address={address}&startblock={start_block}&endblock={end_block}&page={page}&offset={offset}&sort=asc'\n        print(f'query page {page} -&gt; {url}')\n        resp = requests.get(url).json()\n        if resp['message'] == 'OK':\n            items = resp['result']\n            result.extend([{f: i[f] for f in field_keys} for i in items])\n            if len(items) &lt; offset:\n                break\n        else:\n            break\n        page += 1\n    return result\n\n\nRegister the custom function of DuckDB, note the adjustment of page and offset, only get 1 page of data, no pagination demonstration.\n\n\nCode\nconn = duckdb.connect()\nconn = conn.create_function(blockscout_api_with_fields.__name__, blockscout_api_with_fields, [VARCHAR, VARCHAR, VARCHAR, INTEGER, INTEGER, INTEGER, INTEGER], DuckDBPyType(list[fields]))\nconn.execute(\"\"\"\nCREATE OR REPLACE MACRO blockscout_trxs_with_fields(address, start_block, end_block) as table \n    select blockscout_api_with_fields('account', 'txlist', address, start_block, end_block, 1, 5) as data\n\"\"\")\n\n\nQuery the transaction information of the ETH address\n\nconn.execute(\"\"\"\nwith raw_transactions as (\n    select unnest(data) as trx from blockscout_trxs_with_fields('0x603602E9A2ac7f1E26717C2b2193Fd68f5fafFf6', 20485198, 20490674)\n), flatten_transactions as (\n  select unnest(trx) from raw_transactions\n)\nselect \n  blockNumber as block_number,\n  to_timestamp(timeStamp) as datetime,\n  hash,\n  'from',\n  'to',\n  value\nfrom flatten_transactions\n\"\"\").df()\n\nquery page 1 -&gt; https://eth.blockscout.com/api?module=account&action=txlist&address=0x603602E9A2ac7f1E26717C2b2193Fd68f5fafFf6&startblock=20485198&endblock=20490674&page=1&offset=5&sort=asc\n\n\n\n\n\n\n\n\n\nblock_number\ndatetime\nhash\n'from'\n'to'\nvalue\n\n\n\n\n0\n20485198\n2024-08-09 00:55:23+08:00\n0x16e9d0643ce6bf9bc59d5e6c756a196af2941cefc467...\nfrom\nto\n500000000000000000\n\n\n1\n20488106\n2024-08-09 10:38:47+08:00\n0x3f29ab5ba5779df75aee038cb9d529ab7d7e94ff7277...\nfrom\nto\n500000000000000000\n\n\n2\n20490674\n2024-08-09 19:14:23+08:00\n0xcba85af304112c712c978968ff19fb150cdfd18e1f48...\nfrom\nto\n200000000000000000"
  },
  {
    "objectID": "blog/analyzing_blockchain_data_with_duckdb_1/index.html#footnotes",
    "href": "blog/analyzing_blockchain_data_with_duckdb_1/index.html#footnotes",
    "title": "Analyzing Blockchain Data with DuckDB: Data Preparation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nGet Transactions By Address - Blockscout↩︎\nPython Runction API - DuckDB↩︎\nCREATE MACRO Statement - DuckDB↩︎\nGet Transactions By Address - Blockscout↩︎\nGet Transactions By Address - Blockscout↩︎\nPython Runction API - DuckDB↩︎\nCREATE MACRO Statement - DuckDB↩︎\nWith Clause - DuckDB↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "watsy0007",
    "section": "",
    "text": "Analyzing Blockchain Data with DuckDB: Data Preparation\n\n\n7 min\n\n\n\n\n\n\nAug 10, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfind missing dates with DuckDB\n\n\n5 min\n\n\n\n\n\n\nJul 26, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDuckDB Example\n\n\n1 min\n\n\n\n\n\n\nFeb 17, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/duckdb-example/index.html",
    "href": "blog/duckdb-example/index.html",
    "title": "DuckDB Example",
    "section": "",
    "text": "Test DuckDB features based on quarto.\nAll the code below can be copied and executed in a jupyter notebook.\n\nInstall dependencies\n\n!pip install duckdb jupysql --quiet\n\n\n\nBasic configuration\n\nimport duckdb\n\nconn = duckdb.connect()\n\njupysql configuration\n\n%load_ext sql\n\n%config SqlMagic.autopandas = True\n%config SqlMagic.feedback = 0\n%config SqlMagic.displaycon = True\n%config SqlMagic.displaylimit = 10\n\n%sql conn --alias duckdb-native\n\n\n\nDemo data\nRefer to Jupyter Notebooks\n\n%%sql --save short_trips --no-execute\nSELECT *\nFROM 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-01.parquet'\nWHERE trip_distance &lt; 6.3\n\nSkipping execution...\n\n\n\n%sqlplot histogram --table short_trips --column trip_distance --bins 10 --with short_trips\n\n\n\n\n\n\n\n\n\n\n\n\nReuseCC BY-SA 4.0"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Elven | Head Data Scientist | June 2018 - present"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "About",
    "section": "",
    "text": "Elven | Head Data Scientist | June 2018 - present"
  }
]